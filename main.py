# app_trekking_streamlit.py
# Streamlit-only "ì„œìš¸ íŠ¸ë ˆí‚¹ ì½”ìŠ¤ ì¶”ì²œ" (ê³µê³µë°ì´í„° + ë„¤ì´ë²„ ê²€ìƒ‰ í¬ë¡¤ë§)
#
# ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬: streamlit, folium, streamlit-folium, pandas, altair, requests, beautifulsoup4
# ì¶”ê°€ íŒŒì¼/JSON ì €ì¥ ì—†ìŒ (ì „ë¶€ ëŸ°íƒ€ì„ ìˆ˜ì§‘ + Streamlit cache)
#
# ì‹¤í–‰:
#   pip install streamlit folium streamlit-folium pandas altair requests beautifulsoup4
#   streamlit run app_trekking_streamlit.py

from __future__ import annotations

import math
import re
import time
from typing import Any, Dict, List, Optional, Tuple

import altair as alt
import folium
import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup
from streamlit_folium import st_folium


# ----------------------------
# ë„¤ì´ë²„ ê²€ìƒ‰(ë¸”ë¡œê·¸/ë‰´ìŠ¤) HTML í¬ë¡¤ë§
# ----------------------------
UA = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/121 Safari/537.36"
    )
}


def _parse_total_count(text: str) -> int:
    m = re.search(r"ì•½\s*([\d,]+)\s*ê±´", text)
    if not m:
        m = re.search(r"([\d,]+)\s*ê±´", text)
    if not m:
        return 0
    return int(m.group(1).replace(",", ""))


@st.cache_data(ttl=60 * 30)
def naver_search(where: str, query: str, topk: int = 5) -> Dict[str, Any]:
    """
    where: 'blog' or 'news'
    return: {total:int, items:[{title, link, snippet}]}
    """
    url = "https://search.naver.com/search.naver"
    params = {"where": where, "query": query, "sm": "tab_opt"}

    try:
        resp = requests.get(url, params=params, headers=UA, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        page_text = soup.get_text(" ", strip=True)
        total = _parse_total_count(page_text)

        items: List[Dict[str, str]] = []

        if where == "blog":
            links = soup.select(".api_txt_lines.total_tit") or soup.select(
                "a.api_txt_lines"
            )
            descs = soup.select(".api_txt_lines.dsc_txt")
        else:
            links = (
                soup.select("a.news_tit")
                or soup.select(".api_txt_lines.total_tit")
                or soup.select("a.api_txt_lines")
            )
            descs = soup.select(".news_dsc .dsc_txt_wrap") or soup.select(
                ".api_txt_lines.dsc_txt"
            )

        for a in links[:topk]:
            title = a.get_text(strip=True)
            link = a.get("href", "")
            if title:
                items.append({"title": title, "link": link, "snippet": ""})

        for i, d in enumerate(descs[:topk]):
            if i < len(items):
                items[i]["snippet"] = d.get_text(strip=True)[:180]

        return {"total": total, "items": items}
    except Exception as e:
        return {"total": 0, "items": [], "error": str(e)}


def popularity_score(blog_total: int, news_total: int) -> float:
    # ë¸”ë¡œê·¸ê°€ ì½”ìŠ¤ ì²´ê°/í›„ê¸°ì™€ ë” ì§ì ‘ì ì´ë¼ ê°€ì¤‘ì¹˜ â†‘
    return float(round(math.log1p(blog_total) * 1.0 + math.log1p(news_total) * 0.6, 3))


# ----------------------------
# ê³µê³µë°ì´í„°: VWorld(êµ­í† ë¶€/ì‚°ë¦¼ì²­) "ë“±ì‚°ë¡œ" API
# - 2D ë°ì´í„° API 2.0 / ë°ì´í„°: LT_L_FRSTCLIMB
# - ìš”ì²­ URL: https://api.vworld.kr/req/data
# - ì£¼ìš” í•„ë“œ: mntn_nm(ì‚°ëª…), cat_nam(ë‚œì´ë„ ìƒ/ì¤‘/í•˜), sec_len(ê±°ë¦¬m), up_min/down_min(ë¶„)
# ----------------------------
VWORLD_URL = "https://api.vworld.kr/req/data"


def bbox_from_center(
    lat: float, lon: float, radius_km: float
) -> Tuple[float, float, float, float]:
    """
    ëŒ€ì¶© 1ë„ ~ 111km ê°€ì •í•œ ê°„ë‹¨ bbox (ì„œìš¸ ê·œëª¨ì—ì„œëŠ” ì¶©ë¶„íˆ MVPìš©)
    returns: (minx, miny, maxx, maxy) = (lon_min, lat_min, lon_max, lat_max)
    """
    d = radius_km / 111.0
    return (lon - d, lat - d, lon + d, lat + d)


@st.cache_data(ttl=60 * 60)
def vworld_get_trails(
    api_key: str, bbox: Tuple[float, float, float, float], size: int = 1000
) -> List[Dict[str, Any]]:
    """
    bbox: (minx, miny, maxx, maxy) in EPSG:4326
    return: list of raw features (geojson-like)
    """
    minx, miny, maxx, maxy = bbox
    params = {
        "service": "data",
        "version": "2.0",
        "request": "GetFeature",
        "format": "json",
        "data": "LT_L_FRSTCLIMB",
        "key": api_key,
        "geomFilter": f"BOX({minx},{miny},{maxx},{maxy})",
        "size": str(size),
        "page": "1",
        "geometry": "true",
        "attribute": "true",
        "crs": "EPSG:4326",
    }

    resp = requests.get(VWORLD_URL, params=params, timeout=20)
    resp.raise_for_status()
    data = resp.json()

    if data.get("response", {}).get("status") != "OK":
        # NOT_FOUNDë„ ì—¬ê¸°ë¡œ ì˜´
        return []

    # response.result.featureCollection.features
    fc = data["response"]["result"]["featureCollection"]
    return fc.get("features", [])


def _safe_float(x: Any, default: float = 0.0) -> float:
    try:
        return float(x)
    except Exception:
        return default


def _safe_int(x: Any, default: int = 0) -> int:
    try:
        return int(float(x))
    except Exception:
        return default


def normalize_feature(f: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    feature -> record
    geometry: LineString coords [[lon,lat],...]
    """
    props = f.get("properties") or {}
    geom = f.get("geometry") or {}
    if geom.get("type") != "LineString":
        return None

    coords = geom.get("coordinates") or []
    if len(coords) < 2:
        return None

    mntn_nm = (props.get("mntn_nm") or "").strip()
    cat_nam = (props.get("cat_nam") or "").strip()  # ìƒ/ì¤‘/í•˜
    sec_len = _safe_float(props.get("sec_len"), 0.0)
    up_min = _safe_int(props.get("up_min"), 0)
    down_min = _safe_int(props.get("down_min"), 0)

    if not mntn_nm:
        mntn_nm = "ì´ë¦„ì—†ìŒ"

    # ëŒ€í‘œ í¬ì¸íŠ¸(ëì ) = ë§ˆì§€ë§‰ ì¢Œí‘œ
    end_lon, end_lat = coords[-1][0], coords[-1][1]

    return {
        "mntn_nm": mntn_nm,
        "difficulty_raw": cat_nam or "ë¯¸ìƒ",
        "sec_len_m": sec_len,
        "up_min": up_min,
        "down_min": down_min,
        "coords": coords,
        "end_lat": end_lat,
        "end_lon": end_lon,
    }


def difficulty_label(raw: str) -> str:
    # VWorld: ìƒ/ì¤‘/í•˜
    raw = (raw or "").strip()
    if raw == "ìƒ":
        return "ì–´ë ¤ì›€(ìƒ)"
    if raw == "ì¤‘":
        return "ë³´í†µ(ì¤‘)"
    if raw == "í•˜":
        return "ì‰¬ì›€(í•˜)"
    return "ë¯¸ìƒ"


def aggregate_courses(records: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    ì‚°ëª… + ë‚œì´ë„ ê¸°ì¤€ìœ¼ë¡œ ì½”ìŠ¤ í›„ë³´ ìƒì„±
    - ê¸¸ì´ëŠ” êµ¬ê°„ê±°ë¦¬ í•©
    - ëŒ€í‘œ ë¼ì¸ì€ ê°€ì¥ ê¸´ êµ¬ê°„ì˜ ë¼ì¸(ì‹œê°í™”ìš©)
    """
    if not records:
        return pd.DataFrame()

    df = pd.DataFrame(records)
    df["difficulty"] = df["difficulty_raw"].apply(difficulty_label)

    # ì½”ìŠ¤ ID: ì‚°ëª… + ë‚œì´ë„
    df["course_id"] = df["mntn_nm"] + " | " + df["difficulty"]

    # ëŒ€í‘œ ë¼ì¸ = sec_len_m ìµœëŒ€ì¸ feature
    idx = df.groupby("course_id")["sec_len_m"].idxmax()
    rep = df.loc[idx, ["course_id", "coords", "end_lat", "end_lon"]].set_index(
        "course_id"
    )

    agg = df.groupby("course_id", as_index=False).agg(
        mntn_nm=("mntn_nm", "first"),
        difficulty=("difficulty", "first"),
        total_len_m=("sec_len_m", "sum"),
        total_up_min=("up_min", "sum"),
        total_down_min=("down_min", "sum"),
        segments=("sec_len_m", "count"),
    )
    agg["total_len_km"] = (agg["total_len_m"] / 1000.0).round(2)
    agg["est_time_min"] = (agg["total_up_min"] + agg["total_down_min"]).astype(int)

    # rep merge
    agg = agg.set_index("course_id").join(rep).reset_index()

    return agg


@st.cache_data(ttl=60 * 30)
def score_course_row(course_id: str, mntn_nm: str, area_hint: str) -> Dict[str, Any]:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ í¬ë¡¤ë§ ê·¼ê±° ë§Œë“¤ê¸°
    """
    q = f"{area_hint} {mntn_nm} ë“±ì‚°ë¡œ íŠ¸ë ˆí‚¹"
    blog = naver_search("blog", q, topk=5)
    news = naver_search("news", q, topk=5)
    score = popularity_score(
        int(blog.get("total", 0) or 0), int(news.get("total", 0) or 0)
    )
    return {"query": q, "blog": blog, "news": news, "score": score}


# ----------------------------
# íŠ¸ë ˆí‚¹ í›„ ì¹´í˜/ë§¥ì£¼: OSM Overpass (ì˜µì…˜)
# ----------------------------
OVERPASS_URL = "https://overpass-api.de/api/interpreter"


def overpass_query(lat: float, lon: float, radius_m: int) -> List[Dict[str, Any]]:
    query = f"""
    [out:json][timeout:25];
    (
      node(around:{radius_m},{lat},{lon})[amenity=cafe];
      node(around:{radius_m},{lat},{lon})[amenity=bar];
      node(around:{radius_m},{lat},{lon})[amenity=pub];
    );
    out body;
    """
    r = requests.post(OVERPASS_URL, data=query.encode("utf-8"), headers=UA, timeout=30)
    r.raise_for_status()
    return (r.json() or {}).get("elements", [])


def haversine_m(lat1, lon1, lat2, lon2) -> float:
    R = 6371000.0
    p = math.pi / 180
    dlat = (lat2 - lat1) * p
    dlon = (lon2 - lon1) * p
    a = (math.sin(dlat / 2) ** 2) + math.cos(lat1 * p) * math.cos(lat2 * p) * (
        math.sin(dlon / 2) ** 2
    )
    return 2 * R * math.asin(math.sqrt(a))


def extract_place(
    el: Dict[str, Any], origin_lat: float, origin_lon: float
) -> Optional[Dict[str, Any]]:
    if el.get("type") != "node":
        return None
    tags = el.get("tags") or {}
    name = tags.get("name")
    if not name:
        return None

    amenity = tags.get("amenity", "")
    category = "coffee" if amenity == "cafe" else "beer"

    lat = el.get("lat")
    lon = el.get("lon")
    if lat is None or lon is None:
        return None

    dist = int(haversine_m(origin_lat, origin_lon, float(lat), float(lon)))

    # í’ˆì§ˆì ìˆ˜: ì´ë¦„/ì›¹ì‚¬ì´íŠ¸/ì˜ì—…ì‹œê°„ ë“± ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±(0~5)
    quality = 0
    if tags.get("opening_hours"):
        quality += 2
    if tags.get("website") or tags.get("contact:website"):
        quality += 2
    if tags.get("addr:street") or tags.get("addr:full"):
        quality += 1
    quality = min(5, quality)

    return {
        "name": name,
        "category": category,
        "lat": float(lat),
        "lon": float(lon),
        "distance_m": dist,
        "opening_hours": tags.get("opening_hours", ""),
        "website": tags.get("website") or tags.get("contact:website") or "",
        "quality_score": quality,
    }


@st.cache_data(ttl=60 * 30)
def places_near(lat: float, lon: float, radius_m: int) -> List[Dict[str, Any]]:
    try:
        elements = overpass_query(lat, lon, radius_m)
    except Exception:
        return []

    places = [p for p in (extract_place(el, lat, lon) for el in elements) if p]
    for p in places:
        dist_score = 1 - (p["distance_m"] / max(1, radius_m))
        p["combined_score"] = round(
            dist_score * 0.6 + (p["quality_score"] / 5) * 0.4, 3
        )
    places.sort(key=lambda x: x["combined_score"], reverse=True)
    return places


def render_links(title: str, items: List[Dict[str, Any]]) -> None:
    st.markdown(f"**{title}**")
    if not items:
        st.caption("ê²°ê³¼ ì—†ìŒ")
        return
    for it in items[:5]:
        t = it.get("title", "(ì œëª©ì—†ìŒ)")
        link = it.get("link", "")
        snip = it.get("snippet", "")
        if link:
            st.markdown(f"- [{t}]({link})")
        else:
            st.markdown(f"- {t}")
        if snip:
            st.caption(snip)


# ----------------------------
# Streamlit UI
# ----------------------------
st.set_page_config(
    page_title="ì„œìš¸ íŠ¸ë ˆí‚¹ ì¶”ì²œ (ê³µê³µë°ì´í„°+í¬ë¡¤ë§)", page_icon="ğŸ¥¾", layout="wide"
)

st.title("ğŸ¥¾ ì„œìš¸ íŠ¸ë ˆí‚¹ ì½”ìŠ¤ ì¶”ì²œ")
st.caption(
    "ê³µê³µë°ì´í„°(ë“±ì‚°ë¡œ) + ë„¤ì´ë²„ ë¸”ë¡œê·¸/ë‰´ìŠ¤ í¬ë¡¤ë§ ê·¼ê±°ë¡œ ë‚œì´ë„/ì§€ì—­ë³„ Top ì½”ìŠ¤ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤."
)

with st.sidebar:
    st.header("1) ê³µê³µë°ì´í„° ì„¤ì •")
    st.caption("VWorld(ë¸Œì´ì›”ë“œ) ë“±ì‚°ë¡œ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    api_key = st.text_input(
        "VWorld API Key", type="password", placeholder="ë°œê¸‰ë°›ì€ í‚¤ë¥¼ ì…ë ¥"
    )

    st.header("2) ì§€ì—­ ì„ íƒ")
    # MVP: ì¤‘ì‹¬+ë°˜ê²½ ë°©ì‹ (êµ¬ ê²½ê³„ ì—†ì´ë„ 'ì§€ì—­ ì„ íƒ' ëŠë‚Œ êµ¬í˜„)
    preset = st.selectbox(
        "í”„ë¦¬ì…‹ ì§€ì—­(ì¤‘ì‹¬ì )",
        [
            "ì„œìš¸ ì „ì²´(ëŒ€ëµ)",
            "ë‚¨ì‚°/ìš©ì‚°ê¶Œ",
            "ë¶í•œì‚°ê¶Œ(ì€í‰/ê°•ë¶/ë„ë´‰)",
            "í•œê°•/ì—¬ì˜ë„ê¶Œ",
            "ê°•ë‚¨/ì–‘ì¬ê¶Œ",
            "ì‚¬ìš©ì ì§€ì •(ìœ„ê²½ë„)",
        ],
    )

    if preset == "ì‚¬ìš©ì ì§€ì •(ìœ„ê²½ë„)":
        lat = st.number_input("ì¤‘ì‹¬ ìœ„ë„(lat)", value=37.5665, format="%.6f")
        lon = st.number_input("ì¤‘ì‹¬ ê²½ë„(lon)", value=126.9780, format="%.6f")
        radius_km = st.slider("ë°˜ê²½(km)", 2.0, 20.0, 8.0, 0.5)
        area_hint = "ì„œìš¸"
    else:
        # ê°„ë‹¨ í”„ë¦¬ì…‹ ì¤‘ì‹¬ì ë“¤
        presets = {
            "ì„œìš¸ ì „ì²´(ëŒ€ëµ)": (37.5665, 126.9780, 18.0, "ì„œìš¸"),
            "ë‚¨ì‚°/ìš©ì‚°ê¶Œ": (37.5512, 126.9882, 7.0, "ì„œìš¸ ë‚¨ì‚° ìš©ì‚°"),
            "ë¶í•œì‚°ê¶Œ(ì€í‰/ê°•ë¶/ë„ë´‰)": (37.6584, 126.9800, 10.0, "ì„œìš¸ ë¶í•œì‚°"),
            "í•œê°•/ì—¬ì˜ë„ê¶Œ": (37.5250, 126.9250, 9.0, "ì„œìš¸ í•œê°• ì—¬ì˜ë„"),
            "ê°•ë‚¨/ì–‘ì¬ê¶Œ": (37.4840, 127.0350, 8.0, "ì„œìš¸ ê°•ë‚¨ ì–‘ì¬"),
        }
        lat, lon, radius_km, area_hint = presets[preset]

    st.header("3) ë‚œì´ë„/ì •ë ¬")
    diff_filter = st.radio(
        "ë‚œì´ë„", ["ì „ì²´", "ì‰¬ì›€(í•˜)", "ë³´í†µ(ì¤‘)", "ì–´ë ¤ì›€(ìƒ)"], horizontal=False
    )
    topk = st.slider("ì¶”ì²œ ì½”ìŠ¤ ê°œìˆ˜", 3, 10, 3)

    st.header("4) íŠ¸ë ˆí‚¹ í›„ ì¶”ì²œ")
    near_radius_m = st.slider("ì¢…ë£Œì  ì£¼ë³€ ì¶”ì²œ ë°˜ê²½(m)", 100, 1500, 600, 50)
    sip_choice = st.radio(
        "ì¶”ì²œ ì¢…ë¥˜", ["ì „ì²´", "ì¹´í˜(â˜•)", "ë§¥ì£¼(ğŸº)"], horizontal=True
    )

    st.divider()
    st.caption("âš ï¸ ë„¤ì´ë²„/Overpass/ê³µê³µ APIëŠ” ê°€ë” ëŠë¦¬ê±°ë‚˜ ì œí•œë  ìˆ˜ ìˆì–´ìš”.")
    if st.button("ğŸ”„ ìºì‹œ ì´ˆê¸°í™”", use_container_width=True):
        st.cache_data.clear()
        st.success("ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! ìƒˆë¡œê³ ì¹¨í•˜ë©´ ë‹¤ì‹œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")


if not api_key:
    st.warning("ì™¼ìª½ì—ì„œ VWorld API Keyë¥¼ ì…ë ¥í•˜ë©´ ì½”ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•  ìˆ˜ ìˆì–´ìš”.")
    st.stop()

bbox = bbox_from_center(lat, lon, radius_km)

# with st.status(
#     "ê³µê³µë°ì´í„°ì—ì„œ ë“±ì‚°ë¡œ(íŠ¸ë ˆí‚¹ ì½”ìŠ¤ í›„ë³´) ê°€ì ¸ì˜¤ëŠ” ì¤‘â€¦", expanded=False
# ) as status:
#     try:
#         feats = vworld_get_trails(api_key, bbox)
#     except Exception as e:
#         st.error(f"VWorld í˜¸ì¶œ ì‹¤íŒ¨: {e}")
#         st.stop()

#     records = []
#     for f in feats:
#         r = normalize_feature(f)
#         if r:
#             records.append(r)

#     courses = aggregate_courses(records)
#     status.update(label=f"ì½”ìŠ¤ í›„ë³´ ìƒì„± ì™„ë£Œ ({len(courses)}ê°œ)", state="complete")

# if courses.empty:
#     st.info(
#         "ì„ íƒí•œ ì§€ì—­ì—ì„œ ì½”ìŠ¤ í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë°˜ê²½ì„ ëŠ˜ë¦¬ê±°ë‚˜ ë‹¤ë¥¸ ì§€ì—­ì„ ì„ íƒí•´ ë³´ì„¸ìš”."
#     )
#     st.stop()

# ë‚œì´ë„ í•„í„°
if diff_filter != "ì „ì²´":
    courses = courses[courses["difficulty"] == diff_filter].copy()

if courses.empty:
    st.info("í•´ë‹¹ ë‚œì´ë„ì—ì„œ ì½”ìŠ¤ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë‚œì´ë„ë¥¼ ì„ íƒí•´ ë³´ì„¸ìš”.")
    st.stop()

# ì½”ìŠ¤ë³„ ë„¤ì´ë²„ ê·¼ê±° ì ìˆ˜ (ìƒìœ„ë§Œ)
# - ì „ë¶€ í¬ë¡¤ë§í•˜ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ì„œ, ì¼ë‹¨ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ 30ê°œë§Œ ì ìˆ˜ ê³„ì‚°
courses = (
    courses.sort_values("total_len_m", ascending=False).head(30).reset_index(drop=True)
)

with st.status("ë„¤ì´ë²„ ë¸”ë¡œê·¸/ë‰´ìŠ¤ì—ì„œ ì¸ê¸° ê·¼ê±° ìˆ˜ì§‘ ì¤‘â€¦", expanded=False) as status:
    scores = []
    for _, row in courses.iterrows():
        ev = score_course_row(row["course_id"], row["mntn_nm"], area_hint)
        scores.append(ev)
        time.sleep(0.15)  # ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨/ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆì–´ ì‚´ì§ ì‰¬ì–´ì¤Œ
    status.update(label="ê·¼ê±° ìˆ˜ì§‘ ì™„ë£Œ", state="complete")

# evidence merge
courses["query"] = [x["query"] for x in scores]
courses["blog_total"] = [x["blog"].get("total", 0) for x in scores]
courses["news_total"] = [x["news"].get("total", 0) for x in scores]
courses["popularity"] = [x["score"] for x in scores]
courses["evidence_blog"] = [x["blog"] for x in scores]
courses["evidence_news"] = [x["news"] for x in scores]

# ìµœì¢… ì ìˆ˜(ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥)
# - ê¸¸ì´ ì¡°ê¸ˆ ë°˜ì˜(ë„ˆë¬´ ì§§ì€ ì½”ìŠ¤ê°€ í•­ìƒ ì´ê¸°ëŠ” ê±¸ ë°©ì§€)
courses["final_score"] = (
    courses["popularity"] * 1.0
    + courses["total_len_km"].apply(lambda x: math.log1p(float(x))) * 0.4
).round(3)

courses = courses.sort_values("final_score", ascending=False).reset_index(drop=True)
top = courses.head(topk).copy()

# ----------------------------
# ì‹œê°í™”: ì§€ë„ + í‘œ + ê·¼ê±° íŒ¨ë„
# ----------------------------
col_map, col_panel = st.columns([1.35, 1])

with col_map:
    st.subheader("ğŸ—ºï¸ ì¶”ì²œ ì½”ìŠ¤ ì§€ë„")
    center = [lat, lon]
    m = folium.Map(location=center, zoom_start=12, tiles="OpenStreetMap")

    # bbox í‘œì‹œ(ëŒ€ëµ)
    minx, miny, maxx, maxy = bbox
    folium.Rectangle(
        bounds=[[miny, minx], [maxy, maxx]], color="#0984e3", weight=2, fill=False
    ).add_to(m)

    line_colors = [
        "#6c5ce7",
        "#00b894",
        "#e17055",
        "#0984e3",
        "#d63031",
        "#e84393",
        "#2d3436",
        "#fdcb6e",
    ]

    for i, row in top.iterrows():
        coords = row["coords"]  # [[lon,lat],...]
        latlon = [[c[1], c[0]] for c in coords]
        color = line_colors[i % len(line_colors)]
        folium.PolyLine(
            latlon,
            color=color,
            weight=6,
            opacity=0.85,
            tooltip=f"{i+1}ìœ„ {row['course_id']}",
        ).add_to(m)

        # ëì  ë§ˆì»¤
        folium.Marker(
            location=[row["end_lat"], row["end_lon"]],
            tooltip=f"{i+1}ìœ„ ì¢…ë£Œì  Â· {row['mntn_nm']} Â· {row['difficulty']}",
            icon=folium.Icon(color="green", icon="flag"),
        ).add_to(m)

    st_folium(m, height=620, width=None)

with col_panel:
    st.subheader(f"ğŸ… ì¶”ì²œ Top {topk}")
    show_cols = [
        "course_id",
        "difficulty",
        "total_len_km",
        "est_time_min",
        "blog_total",
        "news_total",
        "final_score",
    ]
    st.dataframe(top[show_cols], use_container_width=True, hide_index=True)

    # ì°¨íŠ¸(ê²€ìƒ‰ëŸ‰ ë¹„êµ)
    df_long = top[["course_id", "final_score", "blog_total", "news_total"]].melt(
        id_vars=["course_id", "final_score"],
        value_vars=["blog_total", "news_total"],
        var_name="source",
        value_name="count",
    )

    chart = (
        alt.Chart(df_long)
        .mark_bar()
        .encode(
            x=alt.X("course_id:N", title="ì½”ìŠ¤"),
            y=alt.Y("count:Q", title="ê²€ìƒ‰ëŸ‰(ì¶”ì •)"),
            column=alt.Column("source:N", title=None),
            tooltip=["course_id", "source", "count", "final_score"],
        )
    )
    st.altair_chart(chart, use_container_width=True)

st.divider()

# ìƒì„¸ ë³´ê¸°
selected = st.selectbox("ìƒì„¸ë¡œ ë³¼ ì½”ìŠ¤ ì„ íƒ", top["course_id"].tolist(), index=0)
row = top[top["course_id"] == selected].iloc[0].to_dict()

st.subheader("ğŸ§¾ ì¶”ì²œ ê·¼ê±°(í¬ë¡¤ë§ ê²°ê³¼)")
st.write(
    {
        "course": row["course_id"],
        "difficulty": row["difficulty"],
        "distance_km": row["total_len_km"],
        "estimated_time_min": row["est_time_min"],
        "query": row["query"],
        "blog_total": int(row["blog_total"]),
        "news_total": int(row["news_total"]),
        "final_score": float(row["final_score"]),
    }
)

render_links("ğŸ§± ë¸”ë¡œê·¸ ìƒìœ„ ê²°ê³¼", (row.get("evidence_blog") or {}).get("items", []))
render_links("ğŸ“° ë‰´ìŠ¤ ìƒìœ„ ê²°ê³¼", (row.get("evidence_news") or {}).get("items", []))

st.divider()

st.subheader("â˜•/ğŸº íŠ¸ë ˆí‚¹ í›„ ì¶”ì²œ TOP 10 (ì¢…ë£Œì  ê¸°ì¤€)")
places = places_near(float(row["end_lat"]), float(row["end_lon"]), int(near_radius_m))

if sip_choice != "ì „ì²´":
    want = "coffee" if "ì¹´í˜" in sip_choice else "beer"
    places = [p for p in places if p.get("category") == want]

if not places:
    st.info("ì£¼ë³€ ì¶”ì²œ ì¥ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë°˜ê²½ì„ ëŠ˜ë ¤ë³´ì„¸ìš”.")
else:
    dfp = pd.DataFrame(places[:10])
    keep = [
        "name",
        "category",
        "distance_m",
        "quality_score",
        "combined_score",
        "opening_hours",
        "website",
    ]
    keep = [c for c in keep if c in dfp.columns]
    st.dataframe(dfp[keep], use_container_width=True, hide_index=True)
